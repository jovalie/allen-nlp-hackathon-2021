{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8184,"status":"ok","timestamp":1632059850803,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"WXBdMe8YBgfW","outputId":"50698ef9-e8b8-4822-c18b-530090b4ccd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers\u003c0.11,\u003e=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 40.4 MB/s \n","\u001b[?25hCollecting pyyaml\u003e=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 40.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 41.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting huggingface-hub\u003e=0.0.12\n","  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.0.12-\u003etransformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003etransformers) (2.4.7)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2021.5.30)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.0.1)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2\n"]}],"source":["!pip3 install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3619,"status":"ok","timestamp":1632059854413,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"BptoeLjwCmjf","outputId":"3657f36b-ac8d-4ded-fda1-7d924fafe74a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting captum\n","  Downloading captum-0.4.0-py3-none-any.whl (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: torch\u003e=1.2 in /usr/local/lib/python3.7/dist-packages (from captum) (1.9.0+cu102)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=1.2-\u003ecaptum) (3.7.4.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u003e=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-\u003ecaptum) (2.4.7)\n","Requirement already satisfied: python-dateutil\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-\u003ecaptum) (2.8.2)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-\u003ecaptum) (0.10.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-\u003ecaptum) (1.3.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler\u003e=0.10-\u003ematplotlib-\u003ecaptum) (1.15.0)\n","Installing collected packages: captum\n","Successfully installed captum-0.4.0\n"]}],"source":["!pip3 install captum"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5907,"status":"ok","timestamp":1632059860318,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"t4c6DhV2CuyA"},"outputs":[],"source":["from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, BertModel,AutoModel\n","from captum.attr import visualization as viz\n","from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n","from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n","import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import bz2\n","import transformers\n","import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1632059860318,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"CKh16tLXJzkF","outputId":"57485eab-682a-4d4f-fe87-d572dd7e4cf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["We will use the GPU: Tesla K80\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('We will use the GPU:', torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28410,"status":"ok","timestamp":1632059888711,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"EP6v52_qD8wP","outputId":"498f6809-dd14-4bb8-ef65-7b71019f1efc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Library setups\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1632059888712,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"F5yxj3wfC_XD"},"outputs":[],"source":["# Constants and paths\n","ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/Allen_NLP_hackathon'\n","predicted_nyt_data = os.path.join(ROOT_PATH, 'Data', 'nyt_predicted_sentences_vishakha.csv')\n","BERT_MODELS = os.path.join(ROOT_PATH, 'Models')"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1632059888712,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"enORoC4pFNfm"},"outputs":[],"source":["def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, tokenizer):\n","\n","    text_ids = tokenizer.encode(text, add_special_tokens=False,max_length=64,truncation=True)\n","    # construct input token ids\n","    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n","    # construct reference token ids \n","    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n","\n","    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n","\n","def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n","    seq_len = input_ids.size(1)\n","    token_type_ids = torch.tensor([[0 if i \u003c= sep_ind else 1 for i in range(seq_len)]], device=device)\n","    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n","    return token_type_ids, ref_token_type_ids\n","\n","def construct_input_ref_pos_id_pair(input_ids):\n","    seq_length = input_ids.size(1)\n","    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n","    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n","    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n","\n","    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n","    return position_ids, ref_position_ids\n","    \n","def construct_attention_mask(input_ids):\n","    return torch.ones_like(input_ids)\n","\n","def custom_forward(inputs):\n","    preds = predict(inputs)\n","    #print(torch.softmax(preds, dim = 1).shape)\n","    #print(torch.softmax(preds, dim = 1))\n","    return preds#torch.softmax(preds, dim = 1)[:, 1]# for negative attribution, torch.softmax(preds, dim = 1)[:, 1] \u003c- for positive attribution\n","\n","def predict(inputs):\n","    #print('model(inputs): ', model(inputs))\n","    return model(inputs)[0]\n","\n","def get_attribution_for_test_set(lig, test_data_set, tokenizer):\n","    words_ls = []\n","    attributions_ls = []\n","    test_set_word_att_dict = {}\n","    \n","    for index, row in test_data_set.iterrows():\n","        clean_text = row[\"sentence1\"]\n","        \n","        input_ids, ref_input_ids, sep_id = construct_input_ref_pair(clean_text, ref_token_id, sep_token_id, cls_token_id, tokenizer)\n","        token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n","        position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n","        attention_mask = construct_attention_mask(input_ids)\n","\n","        indices = input_ids[0].detach().tolist()\n","        all_tokens = tokenizer.convert_ids_to_tokens(indices)\n","        \n","        attributions, delta = lig.attribute(inputs=input_ids,\n","                                    baselines=ref_input_ids,\n","                                    n_steps=5000,\n","                                    internal_batch_size=5,\n","                                    return_convergence_delta=True)\n","        tokenized_sen = tokenizer.tokenize(clean_text)\n","        print(len(tokenized_sen))\n","        tokenized_sen = tokenized_sen[:64]\n","        for i in tokenized_sen:\n","            word = i\n","            words_ls.append(word)\n","            index = tokenized_sen.index(i)+1\n","            attribution = float(sum(attributions[0][index]))\n","            attributions_ls.append(attribution)\n","            \n","    #words_ls_flatten = [item for sublist in words_ls for item in sublist]\n","    #attributions_ls_flatten = [item for sublist in attributions_ls for item in sublist]\n","    \n","    test_set_word_att_dict[\"words\"] = words_ls\n","    test_set_word_att_dict[\"attribution\"] = attributions_ls\n","    \n","    return test_set_word_att_dict"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":591},"executionInfo":{"elapsed":754,"status":"error","timestamp":1632059889462,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"rGe-7hM1I38x"},"outputs":[],"source":["# load model\n","model = BertForSequenceClassification.from_pretrained(BERT_MODELS+\"/anger/\", output_hidden_states=True)\n","model.to(device)\n","model.eval()\n","model.zero_grad()\n","\n","# load tokenizer\n","tokenizer = BertTokenizer.from_pretrained(BERT_MODELS+\"/anger/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1632059889458,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"BysSglOGUtCQ"},"outputs":[],"source":["anger_data = pd.read_csv(predicted_nyt_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1632059889459,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"q7Q1vahZeSW0"},"outputs":[{"data":{"text/plain":["Index(['Unnamed: 0', 'Unnamed: 0.1', 'sentence1', 'anger_prediciton',\n","       'sadness_prediciton'],\n","      dtype='object')"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["anger_data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1632059889460,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"_gGTzrFBdqvz"},"outputs":[{"data":{"text/plain":["250"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["len(anger_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1632059889461,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"DliY8b46hO9f"},"outputs":[],"source":["ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n","sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n","cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1632059889461,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"FLswD6wlgwul"},"outputs":[],"source":["lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\n","test_set_word_att_dict = get_attribution_for_test_set(lig, anger_data[:20], tokenizer)\n","anger_feature_importances_scores_nyt_data = os.path.join(ROOT_PATH, 'Results', 'anger_importance_scores_vishakha_20.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1632059889462,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"-adPQV83tsjP"},"outputs":[],"source":["anger_features_importance_Scores = pd.DataFrame(test_set_word_att_dict).sort_values(by=[\"attribution\"],ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1632059889462,"user":{"displayName":"Vishakha Agrawal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11887669426037593521"},"user_tz":-330},"id":"Vyd0TPbG1qnm"},"outputs":[],"source":["anger_features_importance_Scores.to_csv(anger_feature_importances_scores_nyt_data)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"(Vishakha) Anger feature importance scores NYT.ipynb","provenance":[{"file_id":"1lXToCOntXfhcFo_sqtMpp0dwOSbz3IFJ","timestamp":1632059380495}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}