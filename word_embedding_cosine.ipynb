{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"word_embedding_cosine.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"1AJcd5RFO3-T"},"source":["# !pip install spacy datasets -qq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QbPTvcXH8z7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632072164479,"user_tz":240,"elapsed":28297,"user":{"displayName":"Grace Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjRQTcVjRj9eqHtxs0Fo0thtTJQnB5KQj_TpK4qw=s64","userId":"07603196299035417037"}},"outputId":"d4bffa35-d766-4926-e41d-b9c53f487ce8"},"source":["!pip install git+https://github.com/huggingface/transformers"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ovzr56xa\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-ovzr56xa\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 5.2 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 36.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 62.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (4.8.1)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (4.62.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11.0.dev0) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.11.0.dev0) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.11.0.dev0) (3.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.0.dev0) (7.1.2)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.11.0.dev0-py3-none-any.whl size=2833295 sha256=00de26d329e99588e68a9d59b6c2d5b818c307713273391798df1a9193453557\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-7jgg60u0/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.11.0.dev0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"id":"C06H59afFtb7","executionInfo":{"status":"error","timestamp":1632072130275,"user_tz":240,"elapsed":5230,"user":{"displayName":"Grace Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjRQTcVjRj9eqHtxs0Fo0thtTJQnB5KQj_TpK4qw=s64","userId":"07603196299035417037"}},"outputId":"3e4a366d-5c7b-49e5-b362-fedf9e691f62"},"source":["import os\n","import torch\n","import pandas as pd\n","import spacy\n","from spacy.tokenizer import Tokenizer\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    PretrainedConfig,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n",")\n","\n","# Library setups\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","nlp = spacy.load(\"en_core_web_sm\")"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9a24b25f76a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m from transformers import (\n\u001b[1;32m      9\u001b[0m     \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"wMGBEhRjIx31"},"source":["# model = BERTEmotionModel()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnXyVSdmOzca","executionInfo":{"status":"ok","timestamp":1632061912856,"user_tz":240,"elapsed":9,"user":{"displayName":"Grace Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggb9-FFO5yDF1TFhnQb4uQoqCtCIPN9BhzdFFCrxg=s64","userId":"17446548905444720288"}},"outputId":"ab00c6cf-881e-430b-86e9-bcbd8c17d1f4"},"source":["%cd drive/MyDrive/Allen_NLP_hackathon"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/Allen_NLP_hackathon'\n","/content/drive/.shortcut-targets-by-id/18wIy9U4zK_LdoQmnfHm1GL3Xzxy8Ki6V/Allen_NLP_hackathon\n"]}]},{"cell_type":"code","metadata":{"id":"zV2_zgA3g8BZ"},"source":["# given list of sentences \n","def get_word_embedding(sentence, word, model, tokenizer):\n","  sentence = sentence.lower()\n","  word = word.lower()\n","  tokenized_text = tokenizer.tokenize(sentence)\n","  word_index = tokenized_text.index(word)\n","  ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n","  ids = torch.as_tensor([ids, ])\n","  model_output = model.forward(input_ids=ids,output_hidden_states=True)\n","  last_state = model_output.hidden_states[-1]\n","  \n","  # Collapsing the tensor into 1-dimension\n","  token_embeddings = torch.squeeze(last_state, dim=0)\n","  token_embeddings = list(token_embeddings)\n","\n","  word_embedding = token_embeddings[word_index].detach().numpy()\n","\n","  return word_embedding\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VZwzoPeNhVhG"},"source":["model = AutoModelForSequenceClassification.from_pretrained('Models/anger')\n","tokenizer = AutoTokenizer.from_pretrained('Models/anger')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w6RIHgWjfwnC"},"source":["word = 'test'\n","sentence = 'This is a test sentence!'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9AOJbghhY6B"},"source":["emb = get_word_embedding(sentence, word, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s8zaTFvGiB8n"},"source":["import pandas as pd\n","from scipy.spatial.distance import cosine"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWya6ypbiPhX"},"source":["texts = [\n","  \"Girl\",\n","  \"The girl walked to the bank.\",\n","  \"This girl's favorite class was Chemistry.\",\n","  \"The girl with the long black hair is named Jessica.\",\n","  \"I'm a girl, don't mistake me for a boy!\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F9ZKi7l0ikeJ"},"source":["target_word_embeddings = [get_word_embedding(sentence, \"Girl\", model, tokenizer) for sentence in texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVLAmIxNiDvG"},"source":["def gen_cosine_df(texts, target_word_embeddings):\n","  list_of_distances = []\n","  for text1, embed1 in zip(texts, target_word_embeddings):\n","      for text2, embed2 in zip(texts, target_word_embeddings):\n","          cos_dist = 1 - cosine(embed1, embed2)\n","          list_of_distances.append([text1, text2, cos_dist])\n","\n","  distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'cosine'])\n","  return distances_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFM1f8sUi6zq","colab":{"base_uri":"https://localhost:8080/","height":824},"executionInfo":{"status":"ok","timestamp":1632062105507,"user_tz":240,"elapsed":118,"user":{"displayName":"Grace Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggb9-FFO5yDF1TFhnQb4uQoqCtCIPN9BhzdFFCrxg=s64","userId":"17446548905444720288"}},"outputId":"09849df3-f45c-4924-cdbc-64035a424bd1"},"source":["distances_df = gen_cosine_df(texts, target_word_embeddings)\n","distances_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text1</th>\n","      <th>text2</th>\n","      <th>cosine</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Girl</td>\n","      <td>Girl</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Girl</td>\n","      <td>The girl walked to the bank.</td>\n","      <td>0.996245</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Girl</td>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>0.993681</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Girl</td>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>0.995054</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Girl</td>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>0.991203</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>The girl walked to the bank.</td>\n","      <td>Girl</td>\n","      <td>0.996245</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>The girl walked to the bank.</td>\n","      <td>The girl walked to the bank.</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>The girl walked to the bank.</td>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>0.998299</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>The girl walked to the bank.</td>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>0.998938</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>The girl walked to the bank.</td>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>0.995956</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>Girl</td>\n","      <td>0.993681</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>The girl walked to the bank.</td>\n","      <td>0.998299</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>0.999234</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>0.997903</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>Girl</td>\n","      <td>0.995054</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>The girl walked to the bank.</td>\n","      <td>0.998938</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>0.999234</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>0.996860</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>Girl</td>\n","      <td>0.991203</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>The girl walked to the bank.</td>\n","      <td>0.995956</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>This girl's favorite class was Chemistry.</td>\n","      <td>0.997903</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>The girl with the long black hair is named Jes...</td>\n","      <td>0.996860</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>I'm a girl, don't mistake me for a boy!</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text1  ...    cosine\n","0                                                Girl  ...  1.000000\n","1                                                Girl  ...  0.996245\n","2                                                Girl  ...  0.993681\n","3                                                Girl  ...  0.995054\n","4                                                Girl  ...  0.991203\n","5                        The girl walked to the bank.  ...  0.996245\n","6                        The girl walked to the bank.  ...  1.000000\n","7                        The girl walked to the bank.  ...  0.998299\n","8                        The girl walked to the bank.  ...  0.998938\n","9                        The girl walked to the bank.  ...  0.995956\n","10          This girl's favorite class was Chemistry.  ...  0.993681\n","11          This girl's favorite class was Chemistry.  ...  0.998299\n","12          This girl's favorite class was Chemistry.  ...  1.000000\n","13          This girl's favorite class was Chemistry.  ...  0.999234\n","14          This girl's favorite class was Chemistry.  ...  0.997903\n","15  The girl with the long black hair is named Jes...  ...  0.995054\n","16  The girl with the long black hair is named Jes...  ...  0.998938\n","17  The girl with the long black hair is named Jes...  ...  0.999234\n","18  The girl with the long black hair is named Jes...  ...  1.000000\n","19  The girl with the long black hair is named Jes...  ...  0.996860\n","20            I'm a girl, don't mistake me for a boy!  ...  0.991203\n","21            I'm a girl, don't mistake me for a boy!  ...  0.995956\n","22            I'm a girl, don't mistake me for a boy!  ...  0.997903\n","23            I'm a girl, don't mistake me for a boy!  ...  0.996860\n","24            I'm a girl, don't mistake me for a boy!  ...  1.000000\n","\n","[25 rows x 3 columns]"]},"metadata":{},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"HXdZUT1XWv8v"},"source":["## Calculate WEAT test and significance"]},{"cell_type":"code","metadata":{"id":"-v2gYViBamEl"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TriURBpXX93U"},"source":["# text with girl replaced with boy\n","boy_text = [\n","  \"Boy\",\n","  \"The boy walked to the bank.\",\n","  \"This boy's favorite class was Chemistry.\",\n","  \"The boy with the long black hair is named Jessica.\",\n","  \"I'm a boy, don't mistake me for a boy!\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPVgFblAXxBF"},"source":["# target sets with girl, and girl replaced with boy\n","target_girl = target_word_embeddings\n","target_boy = [get_word_embedding(sentence, \"Boy\", model, tokenizer) for sentence in boy_text]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhscUlYHbiby"},"source":["# attribute sets -- will need to change\n","# maybe change to anger attributes next time \n","attr_caucasian = [\"caucasian\", \"caucasian\", \"white\", \"america\", \"america\", \"europe\"]\n","attr_asian = [\"asian\", \"asia\", \"china\", \"asia\"]\n","\n","# turn this into statistical test "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wbPzC-m-W1mF","colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"status":"error","timestamp":1632061936807,"user_tz":240,"elapsed":204,"user":{"displayName":"Grace Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggb9-FFO5yDF1TFhnQb4uQoqCtCIPN9BhzdFFCrxg=s64","userId":"17446548905444720288"}},"outputId":"a1fe2073-7d03-4df3-a905-c3eab9553d94"},"source":["# measures association of word embedding w\n","# with attribute sets A and B\n","def s(w, A, B, model, tokenizer):\n","    # mean_cos_A = np.mean([cosine(w, a) for a in A]) # might need to do attr A\n","    # mean_cos_B = np.mean([cosine(w, b) for b in B])\n","\n","    embds_A = [get_word_embedding(sent_A, w, model, tokenizer) for sent_A in A]\n","    mean_cos_A = df_A.cosine.mean()\n","\n","    embds_B = [get_word_embedding(sent_B, w, model, tokenizer) for sent_B in B]\n","    mean_cos_A = df_B.cosine.mean()\n","    return mean_cos_A - mean_cos_B\n","\n","s_girl = s(target_girl, attr_caucasian, attr_asian, model, tokenizer)\n","s_girl"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-88-03cc9f1bded1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean_cos_A\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_cos_B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0ms_girl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_girl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_caucasian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_asian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0ms_girl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-88-03cc9f1bded1>\u001b[0m in \u001b[0;36ms\u001b[0;34m(w, A, B, model, tokenizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# mean_cos_A = np.mean([cosine(w, a) for a in A]) # might need to do attr A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# mean_cos_B = np.mean([cosine(w, b) for b in B])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0membds_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_word_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent_A\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_cosine_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmean_cos_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-88-03cc9f1bded1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# mean_cos_A = np.mean([cosine(w, a) for a in A]) # might need to do attr A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# mean_cos_B = np.mean([cosine(w, b) for b in B])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0membds_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_word_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent_A\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_cosine_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmean_cos_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-74-735dcb574935>\u001b[0m in \u001b[0;36mget_word_embedding\u001b[0;34m(sentence, word, model, tokenizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_word_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"]}]},{"cell_type":"code","metadata":{"id":"y9868eBBXBT6"},"source":["# WEAT test statistic\n","# target sets X, Y\n","# attribute sets A, B\n","def test_state(X, Y, A, B):\n","  X_strength = np.sum([s(x, A, B) for x in X] )\n","  Y_strength = np.sum([s(y, A, B) for y in Y] )\n","  return X_strength - Y_strength\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XBBV5nXjdvo_"},"source":["P value\n","- one sided p-value of the permutation test is\n","$P[s(X_i, Y_i, A, B) > s(X, Y, A, B)]$\n","\n","Formally, consider a single set of target words\n","W and two sets of attribute words A, B. There is a\n","property $p_w$ associated with each word $w \\in W$.\n","\n","The null hypothesis is that there is no difference between the two sets of target words in terms of their relative similarity to the two sets of attribute words. The permutation test measures the (un)likelihood of the null hypothesis by computing the probability that a random permutation of the attribute words would product the observed (or greater) difference in sample means. \n","- null hypothesis: shuffled data sets should look like real data \n","- otherwise: shuffled data should look different from real data"]},{"cell_type":"code","metadata":{"id":"XRBgBSy-bI1O"},"source":["# P value for permutation test \n","# Pr_i[s(X_i, Y_i, A, B) > S(X, Y, A, B)]\n","\n","prob = [] # calculate whether s(X_i, Y_i, A, B) > S(X, Y, A, B) for each i \n","# for s\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l_WIXoNDmUG_"},"source":["# effect size\n","# normalized measure of how separated thetwo disttributions are "],"execution_count":null,"outputs":[]}]}